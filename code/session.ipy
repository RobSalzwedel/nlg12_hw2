# this is an Ipython session (http://ipython.org/)

%run code/hw2.py

# generating the scatter plot for generateDataset(50,sin,0.03):
from numpy import sin; import pylab

data = generateDataset(50,sin,0.03)
scatter(data[0],data[1], marker='+', facecolor='g')
grid()
box(False)
title("generateDataset(50, sin, 0.03)")
savefig("images/generateDataset(50,sin,0.03).png", dpi=(200))
pylab.close('all')          # close the fig


# computing W_{LS}
import pylab as plt

def y(x, w):
    return sum(w[i] * (x ** i) for i in range(len(w)))

(xs, ts) = generateDataset(10, sin, 0.03)
plt.scatter(xs, ts, marker='+', facecolor='g')
plt.plot(xs, sin(xs), label='$\sin$', linewidth=2)

for M in [1,3,5,10]:
    w = OptimizeLS(xs, ts, M)
    vy = np.vectorize(lambda x: y(x, w))
    plt.plot(xs, vy(xs), label='$M={}$'.format(M))

grid(True)
box(False)
legend(loc=0)
title("10 points with $\sigma=0.03$")
show()
savefig("images/Q1.2_sigma=0.03.png", dpi=(200))
pylab.close('all')          # close the fig

# sigma = 0.1
(xs, ts) = generateDataset(10, sin, 0.1)
plt.scatter(xs, ts, marker='+', facecolor='g')
plt.plot(xs, sin(xs), label='$\sin$', linewidth=2)

for M in [1,3,5,10]:
    w = OptimizeLS(xs, ts, M)
    vy = np.vectorize(lambda x: y(x, w))
    plt.plot(xs, vy(xs), label='$M={}$'.format(M))

grid(True)
box(False)
legend(loc=0)
title("10 points with $\sigma=0.1$")
show()
savefig("images/Q1.2_sigma=0.1.png", dpi=(200))
pylab.close('all')          # close the fig

N = 10
sigma = 0.1
((xt, tt), (xv, tv), (x_tst, t_tst)) = generateDataset3(N, sin, sigma)

lamb_space = np.linspace(-20,5,100) #  100, 1000
errs = {}

for M in [1, 3, 5, 10]:
    errs[M] = {'train': [], 'validate' : [], 'test' : []}
    for log_lambda in lamb_space:
        lamb = np.e ** log_lambda
        W_pls = optimizePLS(xt, tt, M, lamb)
        errs[M]['train'].append(normalized_errs(W_pls, xt, tt))
        errs[M]['validate'].append(normalized_errs(W_pls, xv, tv))
        errs[M]['test'].append(normalized_errs(W_pls, x_tst, t_tst))
    for grp in ['train', 'validate', 'test']:
        plot(lamb_space, errs[M][grp], label='$M={}$ {}'.format(M, grp))
    title("Normalized Errors, M={} N={}".format(M, N))
    xlabel('$\log(\lambda)$')
    # xscale('log')
    grid(True)
    box(False)
    legend(loc=0)
    savefig("images/Q1.3_M={}_N={}_sigma=0.1.png".format(M, N), dpi=(200))
    pylab.close('all')          # close the fig


N = 100
sigma = 0.1
((xt, tt), (xv, tv), (x_tst, t_tst)) = generateDataset3(N, sin, sigma)

lamb_space = np.linspace(-20,5,100) #  100, 1000
errs = {}

for M in [1, 3, 5, 10, 20, 40, 60, 80, 100]:
    errs[M] = {'train': [], 'validate' : [], 'test' : []}
    for log_lambda in lamb_space:
        lamb = np.e ** log_lambda
        W_pls = optimizePLS(xt, tt, M, lamb)
        errs[M]['train'].append(normalized_errs(W_pls, xt, tt))
        errs[M]['validate'].append(normalized_errs(W_pls, xv, tv))
        errs[M]['test'].append(normalized_errs(W_pls, x_tst, t_tst))
    for grp in ['train', 'validate', 'test']:
        plot(lamb_space, errs[M][grp],
             label='$M={}$ {}'.format(M, grp))
    title("Normalized Errors, M={} N={}".format(M, N))
    xlabel('$\log(\lambda)$')
    # xscale('log')
    grid(True)
    box(False)
    legend(loc=0)
    savefig("images/Q1.3_M={}_N={}_sigma=0.1.png".format(M, N),
            dpi=(200))
    pylab.close('all')          # close the fig

# Q1.4 N=10
x10, t10 = generateDataset(10, sin, 0.03)
m, s2 = bayesianEstimator(x10, t10, M=9, alpha=0.005, sigma2=1/11.1)
subplot(111)
upperBound = np.vectorize(lambda x: m(x) + np.sqrt(s2(x)))
lowerBound = np.vectorize(lambda x: m(x) - np.sqrt(s2(x)))
fill_between(x10, upperBound(x10), lowerBound(x10), alpha=0.3, color='r')
scatter(x10, t10, edgecolor='b', facecolor='none', marker='o', s=60, lw=2)
plot(x10, m(x10), label='$m(x)$', lw=2, color='g')
plot(x10, sin(x10), label='$\sin(x)$', lw=2, color='r')
title('$N=10$')
xlabel('$x$')
ylabel('$t$')
legend(loc=2)
savefig('images/bishop_N=10_sin(x)', dpi=(200))
pylab.close('all')          # close the fig


# Q1.4 N=100
x100, t100 = generateDataset(100, sin, 0.03)
m, s2 = bayesianEstimator(x100, t100, M=9, alpha=0.005, sigma2=1/11.1)
subplot(111)
upperBound = np.vectorize(lambda x: m(x) + np.sqrt(s2(x)))
lowerBound = np.vectorize(lambda x: m(x) - np.sqrt(s2(x)))
fill_between(x100, upperBound(x100), lowerBound(x100), alpha=0.3, color='r')
scatter(x100, t100, edgecolor='b', facecolor='none', marker='o', s=60, lw=2)
plot(x100, m(x100), label='$m(x)$', lw=2, color='g')
plot(x100, sin(x100), label='$\sin(x)$', lw=2, color='r')
title('$N=100$')
xlabel('$x$')
ylabel('$t$')
legend(loc=2)
savefig('images/bishop_N=100_sin(x)', dpi=(200))
pylab.close('all')          # close the fig

# Q1.4 N=10 sin(2*pi*x)
x10, t10 = generateDataset(10, lambda x: sin(2*np.pi*x), 0.03)
x100, t100 = generateDataset(100, lambda x: sin(2*np.pi*x), 0.03) # just for result - NOT estimate (smoother graphs)

m, s2 = bayesianEstimator(x10, t10, M=9, alpha=0.005, sigma2=1/11.1)
subplot(111)
upperBound = np.vectorize(lambda x: m(x) + np.sqrt(s2(x)))
lowerBound = np.vectorize(lambda x: m(x) - np.sqrt(s2(x)))
fill_between(x100, upperBound(x100), lowerBound(x100), alpha=0.5, color='pink')
scatter(x10, t10, edgecolor='b', facecolor='none', marker='o', s=60, lw=2)
plot(x100, m(x100), label='$m(x)$', lw=2, color='#5DFC0A')
plot(x100, sin(2*np.pi*x100), label='$\sin(2 \pi x)$', lw=2, color='r')
title('$N=10,\; sin(2 \pi x)$')
xlabel('$x$')
ylabel('$t$')
legend(loc=0)
savefig('images/bishop_N=10_sin(2*pi*x)', dpi=(200))
pylab.close('all')          # close the fig


# Q1.4 N=100
x100, t100 = generateDataset(100, lambda x: sin(2*np.pi*x), 0.03)
m, s2 = bayesianEstimator(x100, t100, M=9, alpha=0.005, sigma2=1/11.1)
subplot(111)
upperBound = np.vectorize(lambda x: m(x) + np.sqrt(s2(x)))
lowerBound = np.vectorize(lambda x: m(x) - np.sqrt(s2(x)))
fill_between(x100, upperBound(x100), lowerBound(x100), alpha=0.5, color='pink')
scatter(x100, t100, edgecolor='b', facecolor='none', marker='o', s=60, lw=2, alpha=0.7)
plot(x100, m(x100), label='$m(x)$', lw=2, color='#5DFC0A')
plot(x100, sin(2*np.pi*x100), label='$\sin(2 \pi x)$', lw=2, color='r')
title('$N=100,\; sin(2 \pi x)$')
xlabel('$x$')
ylabel('$t$')
legend(loc=0)
savefig('images/bishop_N=100_sin(2*pi*x)', dpi=(200))
pylab.close('all')          # close the fig

## Q2
from nltk.corpus import movie_reviews
from itertools import chain

negative = movie_reviews.fileids('neg')
positive = movie_reviews.fileids('pos')


import nltk.classify.NaiveBayesClassifier as naive
from nltk.classify import NaiveBayesClassifier as naive
from nltk.classify.util import accuracy
from nltk.metrics import precision, recall

# first, plot the histograms
pos_fd = nltk.FreqDist([len(rev) for
                        rev in [[review for review in
                                 movie_reviews.sents(fileids=[f])] for
                                f in positive]])

hist(list(chain.from_iterable([[k]*pos_fd[k] for k in pos_fd.keys()])),
     bins=(max(pos_fd.keys()) - min(pos_fd.keys()) + 1))

xlabel('review length (sentences)')
ylabel('number of reviews')
box('off')
grid(True)
title('Length of reviews in positive reviews')
savefig('images/pos_reviews_length.png', dpi=(200))
pylab.close('all')          # close the fig

neg_fd = nltk.FreqDist([len(rev) for
                        rev in [[review for review in
                                 movie_reviews.sents(fileids=[f])] for
                                f in negative]])

hist(list(chain.from_iterable([[k]*neg_fd[k] for k in neg_fd.keys()])),
     bins=(max(neg_fd.keys()) - min(neg_fd.keys()) + 1))

xlabel('review length (sentences)')
ylabel('number of reviews')
box('off')
grid(True)
title('Length of reviews in negative reviews')
savefig('images/neg_reviews_length.png', dpi=(200))
pylab.close('all')          # close the fig

hist(list(chain.from_iterable([[k]*pos_fd[k] for k in pos_fd.keys()])),
     bins=(max(pos_fd.keys()) - min(pos_fd.keys()) + 1),
     label='positive')
hist(list(chain.from_iterable([[k]*neg_fd[k] for k in neg_fd.keys()])),
     bins=(max(neg_fd.keys()) - min(neg_fd.keys()) + 1),
     label='negative')

legend(loc=0)
xlabel('review length (sentences)')
ylabel('number of reviews')
box(False)
grid(True)
title('Positive vs. Negative histogram')
savefig('images/pos_vs_neg_reviews_length.png', dpi=(200))

# splitting by length
# pos_train, pos_test = stratifiedSamples(movie_reviews.sents(fileids=positive))

# neg_train, neg_test = stratifiedSamples(movie_reviews.sents(fileids=negative))

negfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'neg') for
            f in negative]
posfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'pos') for
            f in positive]

shuffle(negfeats)
shuffle(posfeats)
trainfeats = negfeats[:900] + posfeats[:900]
len(trainfeats)                 # 1800
testfeats = negfeats[900:] + posfeats[900:]
len(testfeats)                  # 200

classifier = naive.train(trainfeats)

print 'accuracy: {}'.format(accuracy(classifier, testfeats))
# accuracy: 0.705

# Precision, Recall, F-measure
from collections import defaultdict
refsets = defaultdict(set)
testsets = defaultdict(set)
 
for i, (feats, label) in enumerate(testfeats):
    refsets[label].add(i)
    observed = classifier.classify(feats)
    testsets[observed].add(i)
 
print 'pos precision:', nltk.metrics.precision(refsets['pos'], testsets['pos'])
print 'pos recall:', nltk.metrics.recall(refsets['pos'], testsets['pos'])
print 'pos F-measure:', nltk.metrics.f_measure(refsets['pos'], testsets['pos'])
print 'neg precision:', nltk.metrics.precision(refsets['neg'], testsets['neg'])
print 'neg recall:', nltk.metrics.recall(refsets['neg'], testsets['neg'])
print 'neg F-measure:', nltk.metrics.f_measure(refsets['neg'], testsets['neg'])

## OUTPUT
# pos precision: 0.639455782313
# pos recall: 0.94
# pos F-measure: 0.761133603239
# neg precision: 0.88679245283
# neg recall: 0.47
# neg F-measure: 0.614379084967

classifier.show_most_informative_features()

## OUTPUT
# Most Informative Features
#   maintains = True              pos : neg    =     14.3 : 1.0
#      avoids = True              pos : neg    =     13.0 : 1.0
# outstanding = True              pos : neg    =     12.6 : 1.0
#    dazzling = True              pos : neg    =     12.3 : 1.0
#      seagal = True              neg : pos    =     11.7 : 1.0
#     beliefs = True              pos : neg    =     11.7 : 1.0
#        slip = True              pos : neg    =     11.7 : 1.0
#      elliot = True              pos : neg    =     10.3 : 1.0
#   insulting = True              neg : pos    =      9.8 : 1.0
#       dread = True              pos : neg    =      9.7 : 1.0

